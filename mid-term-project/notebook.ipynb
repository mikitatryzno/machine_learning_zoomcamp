# Recipe Rating Prediction - Exploratory Data Analysis and Model Development

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
import pickle
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

# 1. Load the data
print("Loading data...")
try:
    # Try to load from UCI ML repo
    from ucimlrepo import fetch_ucirepo
    recipe_reviews = fetch_ucirepo(id=911)
    df = recipe_reviews.data.features
    print("Data loaded from UCI ML repository.")
except:
    # Fallback to local file
    df = pd.read_csv('data/recipe_reviews.csv')
    print("Data loaded from local file.")

# 2. Data Exploration
print("\nData Overview:")
print(f"Dataset shape: {df.shape}")
print("\nColumn information:")
print(df.info())

print("\nSample data:")
print(df.head())

print("\nMissing values:")
print(df.isnull().sum())

# 3. Data Preprocessing
print("\nPreprocessing data...")

# Filter out rows with missing text
df = df.dropna(subset=['text'])

# Filter out rows where stars = 0 (no rating)
df_with_ratings = df[df['stars'] > 0]

print(f"Dataset after filtering: {df_with_ratings.shape}")

# Distribution of ratings
plt.figure(figsize=(10, 6))
sns.countplot(x='stars', data=df_with_ratings)
plt.title('Distribution of Recipe Ratings')
plt.xlabel('Star Rating')
plt.ylabel('Count')
plt.savefig('rating_distribution.png')
plt.close()

# 4. Text Preprocessing Function
def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    
    # Remove HTML tags and special characters
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    
    # Tokenize
    tokens = nltk.word_tokenize(text)
    
    # Remove stopwords and lemmatize
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    
    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]
    
    return ' '.join(cleaned_tokens)

# Apply text preprocessing
print("Preprocessing text...")
df_with_ratings['processed_text'] = df_with_ratings['text'].apply(preprocess_text)

# 5. Feature Engineering
print("\nEngineering features...")

# Create features from user metrics
df_with_ratings['interaction_score'] = df_with_ratings['thumbs_up'] - df_with_ratings['thumbs_down']
df_with_ratings['has_replies'] = (df_with_ratings['reply_count'] > 0).astype(int)

# 6. Split the data
print("\nSplitting data into train and test sets...")
X = df_with_ratings[['processed_text', 'user_reputation', 'thumbs_up', 'thumbs_down', 'interaction_score', 'has_replies']]
y = df_with_ratings['stars']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 7. Create preprocessing pipeline
print("\nCreating preprocessing pipeline...")
# Text features
text_features = ['processed_text']
text_transformer = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000, min_df=5))
])

# Numeric features
numeric_features = ['user_reputation', 'thumbs_up', 'thumbs_down', 'interaction_score', 'has_replies']
numeric_transformer = Pipeline([
    ('scaler', StandardScaler())
])

# Combine transformers
preprocessor = ColumnTransformer(
    transformers=[
        ('text', text_transformer, text_features),
        ('num', numeric_transformer, numeric_features)
    ])

# 8. Train models
print("\nTraining models...")

# Logistic Regression
lr_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000, random_state=42))
])

print("Training Logistic Regression...")
lr_pipeline.fit(X_train, y_train)
lr_pred = lr_pipeline.predict(X_test)
lr_accuracy = accuracy_score(y_test, lr_pred)
print(f"Logistic Regression Accuracy: {lr_accuracy:.4f}")
print(classification_report(y_test, lr_pred))

# Random Forest
rf_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

print("\nTraining Random Forest...")
rf_pipeline.fit(X_train, y_train)
rf_pred = rf_pipeline.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)
print(f"Random Forest Accuracy: {rf_accuracy:.4f}")
print(classification_report(y_test, rf_pred))

# Gradient Boosting
gb_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))
])

print("\nTraining Gradient Boosting...")
gb_pipeline.fit(X_train, y_train)
gb_pred = gb_pipeline.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)
print(f"Gradient Boosting Accuracy: {gb_accuracy:.4f}")
print(classification_report(y_test, gb_pred))

# 9. Model Comparison
print("\nModel Comparison:")
models = {
    'Logistic Regression': lr_accuracy,
    'Random Forest': rf_accuracy,
    'Gradient Boosting': gb_accuracy
}

best_model = max(models, key=models.get)
print(f"Best model: {best_model} with accuracy {models[best_model]:.4f}")

# Plot model comparison
plt.figure(figsize=(10, 6))
sns.barplot(x=list(models.keys()), y=list(models.values()))
plt.title('Model Accuracy Comparison')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.savefig('model_comparison.png')
plt.close()

# 10. Confusion Matrix for Best Model
if best_model == 'Logistic Regression':
    best_pred = lr_pred
    best_pipeline = lr_pipeline
elif best_model == 'Random Forest':
    best_pred = rf_pred
    best_pipeline = rf_pipeline
else:
    best_pred = gb_pred
    best_pipeline = gb_pipeline

plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test, best_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title(f'Confusion Matrix - {best_model}')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.savefig('confusion_matrix.png')
plt.close()

# 11. Save the best model
print(f"\nSaving the best model ({best_model})...")
with open('model.pkl', 'wb') as f:
    pickle.dump(best_pipeline, f)

print("Model saved as model.pkl")
print("\nNotebook execution completed.")